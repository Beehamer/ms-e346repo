{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.\n",
    "Jan 11th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Write out the MP/MRP definitions and MRP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts)\n",
    "\n",
    "#### Answer:\n",
    "The Markov Property is such that the future state will be independent from the previous state given the current state is observed.\n",
    "$${P[S_{t+1} | S_t] = P[S_{t+1} | S_t, S_{t-1}, ..., S_1]}$$\n",
    "\n",
    "The Markov Repard Process is a Markov Process with reward. It is consists of a set of ${S, P, R, \\gamma}$  \n",
    "where ${S}$ is a finite set of states, ${P}$ is a state transition probability matrix, ${R}$ is the reward function and ${\\gamma}$ is the discount factor.  \n",
    "\n",
    "The return ${G_t}$ is the total discounted reward from time step t $${G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\Sigma_{k = 0}^{\\infty}\\gamma^k R_{t + k + 1}}$$\n",
    "\n",
    "The value function gives the long-term value of state s: $${v(s) = E[G_t | S_t = s]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Think about the data structures/class design (in Python 3) to represent MP/MRP and implement them with clear type declarations. Remember your data structure/code design must resemble the Mathematical/notational formalism as much as possible. Specifically the data structure/code design of MRP should be incremental (and not independent) to that of MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess:\n",
    "    \"\"\"A class describing a Markov Process\"\"\"\n",
    "        \n",
    "    def get_all_states(self, S):\n",
    "        # return the names of each state in a dictionary\n",
    "        return set(S.keys())\n",
    "    \n",
    "    def construct_row_transition_matrix(self, row):\n",
    "        # using the similar approximation function as MDP-DP-RL where two values are taken \n",
    "        # as the same when the absolute difference between them is smaller than 1e-8\n",
    "        return {s: v for s, v in row.items() if not abs(v - 0.0) <= 1e-8}\n",
    "    \n",
    "    def get_transition_matrix(self, S):\n",
    "        transition_matrix = {s : self.construct_row_transition_matrix(v) for s, v in S.items()}\n",
    "        return transition_matrix\n",
    "            \n",
    "    \n",
    "    def get_stationary_distribution(self):\n",
    "        sz = len(self.all_states_list)\n",
    "        mat = np.zeros((sz, sz))\n",
    "        for i, s1 in enumerate(self.all_states_list):\n",
    "            for j, s2 in enumerate(self.all_states_list):\n",
    "                mat[i, j] = self.transition[s1].get(s2, 0.)\n",
    "\n",
    "        eig_vals, eig_vecs = eig(mat.T)\n",
    "        stat = np.array(\n",
    "            eig_vecs[:, np.where(np.abs(eig_vals - 1.) < 1e-8)[0][0]].flat\n",
    "        ).astype(float)\n",
    "        norm_stat = stat / sum(stat)\n",
    "        return {s: norm_stat[i] for i, s in enumerate(self.all_states_list)}\n",
    "    \n",
    "    \n",
    "    def __init__(self, S):\n",
    "        self.S = S\n",
    "        \n",
    "        # all_states_list is a variable containing all the state names\n",
    "        self.all_states_list = list(self.get_all_states(S))\n",
    "        \n",
    "        # transition is dictionary of the transition matrix, where the keys are the states and the values are dictinaries of current row\n",
    "        # Without type coersion, the following two instantiations are equivalent\n",
    "        \n",
    "#         self.transition = S\n",
    "        self.transition = self.get_transition_matrix(S)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = {\n",
    "    1: {1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2},\n",
    "    2: {1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29},\n",
    "    3: {1: 0.7, 2: 0.3},\n",
    "    4: {1: 0.3, 2: 0.5, 3: 0.2}\n",
    "}\n",
    "a_MP_process = MarkovProcess(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2},\n",
       " 2: {1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29},\n",
       " 3: {1: 0.7, 2: 0.3},\n",
       " 4: {1: 0.3, 2: 0.5, 3: 0.2}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_MP_process.transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yilun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.28574421284173046,\n",
       " 2: 0.38860374986906865,\n",
       " 3: 0.15580810725882485,\n",
       " 4: 0.16984393003037593}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_MP_process.get_stationary_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This class object will be implemented in a python file as well for future uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Separately implement the r(s,s') and the R(s) = \\sum_{s'} p(s,s') * r(s,s') definitions of MRP. Write code to convert/cast the r(s,s') definition of MRP to the R(s) definition of MRP (put some thought into code design here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two ways of constructing the class will be presented\n",
    "# in the first way of constructing the the Markov Reward Process by passing the transition matrix and reward matrix separately\n",
    "class MarkovRewardProcess:\n",
    "    \"\"\" A class describing Markov Reward Process\"\"\"\n",
    "    def get_all_states(self, S):\n",
    "        # return the names of each state in a dictionary\n",
    "        return set(S.keys())\n",
    "    \n",
    "    def construct_row_transition_matrix(self, row):\n",
    "        # using the similar approximation function as MDP-DP-RL where two values are taken \n",
    "        # as the same when the absolute difference between them is smaller than 1e-8\n",
    "        return {s: v for s, v in row.items() if not abs(v - 0.0) <= 1e-8}\n",
    "    \n",
    "    def get_transition_matrix(self, S):\n",
    "        transition_matrix = {s : self.construct_row_transition_matrix(v) for s, v in S.items()}\n",
    "        return transition_matrix\n",
    "    \n",
    "    def get_stationary_distribution(self):\n",
    "        sz = len(self.all_states_list)\n",
    "        mat = np.zeros((sz, sz))\n",
    "        for i, s1 in enumerate(self.all_states_list):\n",
    "            for j, s2 in enumerate(self.all_states_list):\n",
    "                mat[i, j] = self.transition[s1].get(s2, 0.)\n",
    "\n",
    "        eig_vals, eig_vecs = eig(mat.T)\n",
    "        stat = np.array(\n",
    "            eig_vecs[:, np.where(np.abs(eig_vals - 1.) < 1e-8)[0][0]].flat\n",
    "        ).astype(float)\n",
    "        norm_stat = stat / sum(stat)\n",
    "        return {s: norm_stat[i] for i, s in enumerate(self.all_states_list)}\n",
    "    \n",
    "    def __init__(self, transition, reward):\n",
    "        # in this case, the transition matrix and the reward matrix are separate dictionaries\n",
    "        self.transition = transition\n",
    "        self.reward = reward\n",
    "        self.all_state_list = list(get_all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess_matrix:\n",
    "    # the class object holds the transition probability as the matrix so does the reward\n",
    "    def __init__(self, transition, reward):\n",
    "        self.transition = np.matrix(transition)\n",
    "        self.reward = np.matrix(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Reward(p_s, r_s):\n",
    "    # where p_s is a dictionary whose keys are the original state \n",
    "    # and the values are dictionaries of transited states and corresponding probabilities\n",
    "    \n",
    "    Reward = {}\n",
    "    for state in p_s.keys():\n",
    "        Reward[state] = 0\n",
    "        for transitted_state, probability in p_s[state].items():\n",
    "            Reward[state] += probability * r_s[state][transitted_state]\n",
    "    \n",
    "    return Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Write code to generate the stationary distribution for an MP\n",
    "\n",
    "#### Answer\n",
    "See the code for the declaration of the Markov Process and the attribute called get_stationary_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
