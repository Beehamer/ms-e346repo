{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Write code for the interface for RL algorithms with value function approximation. The core of this interface should be a function from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable, Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy approximation for Pi function\n",
    "class Pi_NN(nn.Module):\n",
    "    def __init__(self, input_size, state_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, state_size, bias = True)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# Function approximation for the Q function     \n",
    "class Q_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1, bias = True)\n",
    "        )\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        return out\n",
    "\n",
    "# Function approximation for V function    \n",
    "class V_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1, bias = True)\n",
    "        )\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLFuncApproxBase(OptBase):\n",
    "\n",
    "    NUM_SAMPLES_PER_ACTION = 10\n",
    "\n",
    "    def __init__(self, mdp_rep_for_rl, epsilon: float, num_episodes: int, max_steps: int, fa_spec):\n",
    "\n",
    "        self.mdp_rep: MDPRepForRLFA = mdp_rep_for_rl\n",
    "        \n",
    "        # TODO: epsilon with decay should be allowed\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes: int = num_episodes\n",
    "        self.max_steps: int = max_steps\n",
    "        self.vf_fa: FuncApproxBase = fa_spec.get_vf_func_approx_obj()\n",
    "        self.qvf_fa: FuncApproxBase = fa_spec.get_qvf_func_approx_obj()\n",
    "        self.state_action_func = self.mdp_rep.state_action_func\n",
    "\n",
    "    def get_init_policy_func(self) -> PolicyActDictType:\n",
    "        return get_uniform_policy_func(self.state_action_func)\n",
    "\n",
    "    def get_value_func_fa(self, polf: PolicyActDictType) -> VFType:\n",
    "        qv_func = self.get_qv_func_fa(polf)\n",
    "\n",
    "        # noinspection PyShadowingNames\n",
    "        def vf(s: S, polf=polf, qv_func=qv_func) -> float:\n",
    "            return sum(polf(s)[a] * qv_func(s)(a) for a in\n",
    "                       self.state_action_func(s))\n",
    "\n",
    "        return vf\n",
    "\n",
    "    # noinspection PyShadowingNames\n",
    "    def get_value_func(self, pol_func: PolicyType) -> VFType:\n",
    "        return self.get_value_func_fa(\n",
    "            lambda s, pol_func=pol_func: get_pdf_from_samples(\n",
    "                pol_func(s)(len(self.state_action_func(s)) *\n",
    "                            RLFuncApproxBase.NUM_SAMPLES_PER_ACTION)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_qv_func_fa(self, polf: Optional[PolicyActDictType]) -> QFType:\n",
    "        pass\n",
    "\n",
    "    # noinspection PyShadowingNames\n",
    "    def get_act_value_func(self, pol_func: PolicyType) -> QFType:\n",
    "        return self.get_qv_func_fa(\n",
    "            lambda s, pol_func=pol_func: get_pdf_from_samples(\n",
    "                pol_func(s)(len(self.state_action_func(s)) *\n",
    "                            RLFuncApproxBase.NUM_SAMPLES_PER_ACTION)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_optimal_det_policy_func(self) -> Callable[[S], A]:\n",
    "        qv_func = self.get_qv_func_fa(None)\n",
    "\n",
    "        # noinspection PyShadowingNames\n",
    "        def detp_func(s: S, qv_func=qv_func) -> A:\n",
    "            return max(\n",
    "                [(a, qv_func(s)(a)) for a in self.state_action_func(s)],\n",
    "                key=itemgetter(1)\n",
    "            )[0]\n",
    "\n",
    "        return detp_func\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
