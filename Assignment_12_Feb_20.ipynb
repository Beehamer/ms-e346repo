{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Write code for the interface for RL algorithms with value function approximation. The core of this interface should be a function from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable, Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy approximation for Pi function\n",
    "class Pi_NN(nn.Module):\n",
    "    def __init__(self, input_size, state_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, state_size, bias = True)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# Function approximation for the Q function     \n",
    "class Q_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1, bias = True)\n",
    "        )\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        return out\n",
    "\n",
    "# Function approximation for V function    \n",
    "class V_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1, bias = True)\n",
    "        )\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"\n",
    "    The stochasic policy that maps a state to the transition probability\n",
    "    The policy is a mapping from a state to an action, i.e. given a state, figuring out the probability of selecting some actions\n",
    "    \"\"\"\n",
    "    def __init__(self, data: Dict[S, Mapping[A, float]]) -> None:\n",
    "        self.data = data\n",
    "\n",
    "    def get_state_probabilities(self, state: S) -> Mapping[A, float]:\n",
    "        return self.data[state]\n",
    "\n",
    "    def get_state_action_probability(self, state: S, action: A) -> float:\n",
    "        return self.get_state_probabilities(state).get(action, 0.)\n",
    "\n",
    "    # update state/ action probability using epsilon greedy\n",
    "    def update_state_action_to_epsilon_greedy(self, state: S, action_value_dict: Mapping[A, float], epsilon: float):\n",
    "        max_act = max(action_value_dict.items(), key=itemgetter(1))[0]\n",
    "        if epsilon == 0:\n",
    "            ret = {max_act: 1.}\n",
    "        else:\n",
    "            ret = {a: epsilon / len(action_value_dict) + (1. - epsilon if a == max_act else 0.) for a in action_value_dict.keys()}\n",
    "        self.policy_data[state] = ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.policy_data.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.policy_data.__str__()\n",
    "\n",
    "class RLFuncApproxBase(OptBase):\n",
    "\n",
    "    NUM_SAMPLES_PER_ACTION = 10\n",
    "\n",
    "    def __init__(self, mdp_rep_for_rl, epsilon: float, num_episodes: int, max_steps: int, fa_spec):\n",
    "\n",
    "        self.mdp_rep: MDPRepForRLFA = mdp_rep_for_rl\n",
    "        \n",
    "        # TODO: epsilon with decay should be allowed\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes: int = num_episodes\n",
    "        self.max_steps: int = max_steps\n",
    "        self.vf_fa: FuncApproxBase = V_NN(len(self.mdp_rep.states))\n",
    "        self.qvf_fa: FuncApproxBase = Q_NN(len(self.mdp_rep.states))\n",
    "        # the simulator of getting the next, given the current state and action\n",
    "        self.state_action_func = self.mdp_rep.state_action_func\n",
    "\n",
    "    def get_init_policy_func(self) -> PolicyActDictType:\n",
    "        return Policy({s: {a: 1. / len(v) for a in v} for s, v in self.state_action_dict.items()})\n",
    "\n",
    "    def get_value_func_fa(self, polf: PolicyActDictType) -> VFType:\n",
    "        qv_func = self.vf_fa # the function approximation of the reward in each state\n",
    "        # return the expectation of current states\n",
    "        return sum(polf(s)[a] * qv_func(s)(a) for a in self.state_action_func(s))\n",
    "\n",
    "\n",
    "    def get_value_func(self, pol_func: PolicyType) -> VFType:\n",
    "        return self.get_value_func_fa(lambda s, pol_func=pol_func: get_pdf_from_samples(pol_func(s)(len(self.state_action_func(s)) * RLFuncApproxBase.NUM_SAMPLES_PER_ACTION)))\n",
    "\n",
    "\n",
    "    def get_act_value_func(self, pol_func: PolicyType) -> QFType:\n",
    "        # a weighted average\n",
    "        return self.get_qv_func_fa(\n",
    "            lambda s, pol_func=pol_func: get_pdf_from_samples(\n",
    "                pol_func(s)(len(self.state_action_func(s)) *\n",
    "                            RLFuncApproxBase.NUM_SAMPLES_PER_ACTION)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_optimal_det_policy_func(self) -> Callable[[S], A]:\n",
    "        qv_func = self.get_qv_func_fa(None)\n",
    "\n",
    "        # noinspection PyShadowingNames\n",
    "        def detp_func(s: S, qv_func=qv_func) -> A:\n",
    "            return max(\n",
    "                [(a, qv_func(s)(a)) for a in self.state_action_func(s)],\n",
    "                key=itemgetter(1)\n",
    "            )[0]\n",
    "\n",
    "        return detp_func\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
