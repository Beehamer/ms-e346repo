{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS&E 346 Assignment 5\n",
    "#### January 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Model Merton's Portfolio problem as an MDP (write the model in LaTeX)  \n",
    "Implement this MDP model in code  \n",
    "Try recovering the closed-form solution with a DP algorithm that you implemented previously  \n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The portfolio can be described as follows: one can invest an arbitrary amount of money to an risky asset and a risk-free asset in continous time ${0 \\leq t \\leq T}$, whereas each asset has a known mean of returns and variance. The goal of the constructed portfolio is to maximize the life-time aggregated utility of consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem then could be seen to find the optimal discounted value function. Balancing constraints implies the following process for wealth ${W_t}$ and at any time ${t}$ determine the optimal position to maximize $${E[\\int_{t}^{T}\\frac{e^{-\\rho(s-t)c_s^{1-\\gamma}}}{1-\\gamma} ds + \\frac{e^{-\\rho(T - t)\\epsilon^{\\gamma}W_T^{1-\\gamma}}}{1-\\gamma}]}$$ and assume the Bequest function as ${\\epsilon^\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of this as a continuous-time Stochastic Control problem, with the state ${(t, W_t)}$, and the action is ${[\\pi_t, c_t]}$, and the reward is ${U(c_t)}$. In the following reasoning, we discretize the problem, and it could be seen as in finite steps to maximize the expected return ${[\\pi_t, c_t]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, we state and solve the problem for 1 risky asset\n",
    "# where the random variable dS_t is written in the formula shown above\n",
    "# denote pi as the fraction of the risky asset allocated\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import NamedTuple, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of this as a continous-time stocastic **Control** problem  \n",
    "* The state is ${(t, W_t)}$\n",
    "* the action is ${\\pi_t, c_t}$\n",
    "* the Reward per unit time is ${U(c_t)}$\n",
    "* The Return is the usual accumulated discounted Reward  \n",
    "\n",
    "And the task is to find the optimal policy to maximize the expected return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimal_allocation_consumption:\n",
    "    \"\"\"\n",
    "    This function discreterize the state space and\n",
    "    limit the policy space to be finite\n",
    "    in this toy example, we discretize the W/c value to 10 (0 - 9)\n",
    "    and in terms of policy, we can only invest the multiple of 10% to the risky asset\n",
    "    \"\"\"\n",
    "    def calculate_next_w(self, curt_t, curt_W, action):\n",
    "        (pi, c) = action\n",
    "        next_return_risk = np.random.normal(self.mu, self.sigma, size = None)\n",
    "        next_W = curt_W + ((1 - pi) * self.r + pi * next_return_risk) * curt_W - c * curt_W\n",
    "        # in order to fit in the discretized state space, we make the next_W also a integer\n",
    "        next_W = math.ceil(next_W)\n",
    "        next_t = curt_t + 1\n",
    "        return (next_t, next_W)\n",
    "    \n",
    "    def state_reward_gen(self, state, action, num_samples: int):\n",
    "        t, W = state\n",
    "        next_states = [(\n",
    "            t + 1,\n",
    "            action[0] * (1. + rr) + (W - action[0]) * (1. + self.r)\n",
    "        ) for rr in self.risky_returns(num_samples)]\n",
    "        return [(\n",
    "            x,\n",
    "            self.gamma * self.utility(x[1]) if t == self.time_steps - 1 else 0.\n",
    "        ) for x in next_states]\n",
    "    \n",
    "    # passing in the \n",
    "    def risky_returns(self, size):\n",
    "        return np.random.normal(loc=self.mu, scale=self.sigma, size=size)\n",
    "    \n",
    "    def __init__(self, T, gamma, mu, r, sigma):\n",
    "        self.num_of_money_states = 10\n",
    "        self.num_of_action_states = 11 # (0%, 10%, ..., 100%)\n",
    "        self.num_of_states = T * self.num_of_money_states\n",
    "        self.T = T\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # the mean return/ standard deviation of risky asset\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # the treasure bond rate (risk-free rate)\n",
    "        self.r = r\n",
    "        \n",
    "        # initialize the transition/ policy matrix\n",
    "        self.transition = np.zeros((self.num_of_states, self.num_of_states))\n",
    "        self.policy_matrix = np.zeros(self.num_of_states, self.num_of_action_states)\n",
    "        self.policy_set = list([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
