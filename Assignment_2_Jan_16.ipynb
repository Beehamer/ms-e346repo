{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS&E 346 Assignment 2\n",
    "#### January 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Write the Bellman equation for MRP Value Function and code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "The Bellman equation for MRP value funcion:\n",
    "$${v(s) = R_s + \\gamma \\Sigma_{s' \\in S}p_{ss'}v(s')}$$\n",
    "The Bellman equation can be expressed consisely using matrices:\n",
    "$${v = R + \\gamma Pv}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_v(gamma, P, R):\n",
    "    \"\"\"the function to solve the reward for the markovian reward process\"\"\"\n",
    "    a = np.identity(P.shape[0]) - gamma * P\n",
    "    inv = np.linalg.inv(a)\n",
    "    return np.matmul(inv, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "P = np.matrix([[0.5, 0.5],[1, 0]])\n",
    "R = np.matrix([[0.6, -1], [0.3, -0.5]])\n",
    "v = solve_v(gamma, P, R)\n",
    "print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Write out the MDP definition, Policy definition and MDP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "A Markov Decision Process is a Markov Reward Process with decisions. It is an environment where all states are Markovian. The Markov Decision Process is made of a tuple of a finite set of states, a finite set of actions, a state transition probability, a reward function and a discount factor. It could be written as ${<S, A, P, R \\gamma>}$, where ${P_{ss'}^a = P[S_{t+1} = s' | S_t = s, A_t = a}$\n",
    "\n",
    "A policy ${\\pi}$ is a distribution over actions given states, it is at choice of the agent, i.e. a policy defines the behavior of the agent.\n",
    "$${\\pi(a|s) = P[A_t = a | S_t = s]}$$  \n",
    "\n",
    "The state-value function ${v_\\pi(s)}$ is the expected return starting from the state s, and then following the policy ${\\pi}$, which is to say, assuming, from the current state, all the future state will be determined by the given policy, the expected reward is the state-value function.\n",
    "$${v_\\pi(s) = E_\\pi[G_t | S_t = s]}$$\n",
    "\n",
    "The action value function, ${q_\\pi(s, a)}$ is the expected return from state s and taking action a, and then following the policy ${\\pi}$\n",
    "$${q_\\pi(s, a) = E_\\pi[G_t | S_t = s, A_t = a]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "Think about the data structure/class design (in Python 3) to represent MDP, Policy, Value Function, and implement them with clear type definitions. The data structure/code design of MDP should be incremental (and not independent) to that of MRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess:\n",
    "    \n",
    "    def get_transition_matrix(self, data):\n",
    "        name2index = {}\n",
    "        for i, name_of_state in enumerate(data):\n",
    "            name2index[name_of_state] = i\n",
    "        \n",
    "        self.name2index = name2index\n",
    "        \n",
    "        # store the transition probability in a transition matrix\n",
    "        P = np.zeros((len(name2index), len(name2index)))\n",
    "        for name_of_state in data:\n",
    "            for name_of_next_state in data[name_of_state]:\n",
    "                P[name2index(name_of_state), name2index(name_of_next_state)] = data[name_of_state][name_of_next_state]\n",
    "        print ('The transition matrix is constructed as: ', P)\n",
    "        return P\n",
    "    \n",
    "    def get_value_matrix(self, data):\n",
    "        name2index = self.name2index\n",
    "        V = np.zeros((len(name2index), len(name2index)))\n",
    "        # the case when the reward is only associated with states, as is in MRP\n",
    "        for name_of_state in data:\n",
    "            for name_of_next_state in data[name_of_state]:\n",
    "                V[name2index(name_of_state), name2index(name_of_next_state)] = data[name_of_state][name_of_next_state]\n",
    "    \n",
    "    def get_policy_matrix(self, policy):\n",
    "        name2index = self.name2index\n",
    "        # construct a dictionary to mapping the name of the action to index\n",
    "        action2index = {}\n",
    "        i = 0\n",
    "        for name_of_state in policy:\n",
    "            for name_of_action in policy[name_of_state]:\n",
    "                if name_of_action not in action2index:\n",
    "                    action2index[name_of_action] = i\n",
    "                    i += 1\n",
    "        self.action2index = action2index\n",
    "        \n",
    "        policy_matrix = np.zeros((len(name2index), len(name2index)))\n",
    "        for name_of_state in policy:\n",
    "            for name_of_action in policy[name_of_state]:\n",
    "                policy_matrix[name2index(name_of_state), action2index(name_of_action)] = policy[name_of_state][name_of_action]\n",
    "        return policy_matrix\n",
    "        \n",
    "    def link_model_to_states(self, model):\n",
    "        name2index = self.name2index\n",
    "        action2index = self.action2index\n",
    "        p_ss_prime = np.zeros((len(name2index), len(action2index), len(name2index)))\n",
    "        # for this part, it will link a tuple of <state, action> to next state and generate the corresponding probabilistic distribution\n",
    "        transition = self.transition\n",
    "        policy = self.policy\n",
    "        for state_name in policy:\n",
    "            for action_name in policy[state_name]:\n",
    "                p_curts_curta = policy[state_name][action_name]\n",
    "                p_ss_prime += p_curts_curta * model[next_state_name][state_name][action_name]            \n",
    "        return P_ss_prime\n",
    "    \n",
    "    def convert_to_MRP(self, policy, action, model, gamma, value):\n",
    "        P_ss_prime = link_model_to_states(model)\n",
    "        r_s = np.sum(np.multiply(P_ss_prime, value), axis = 1)\n",
    "        return r_s\n",
    "    \n",
    "    def __init__(self, transition: dict, policy: dict, action: dict, model: dict, value: dict, gamma):\n",
    "        # the transition matrix will be stored in a numpy matrix with transition probability\n",
    "        self.transition = get_transition_matrix(transition)\n",
    "        self.gamma = gamma\n",
    "        self.policy = policy\n",
    "        self.model = model\n",
    "        self.action = action\n",
    "        value = convert_to_MRP(polic, action, model, gamma, value)\n",
    "        self.value = value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The class/ object for MDP can be seen as above, where the transition matrix is a dictionary of the transition probability, policy is a diction of diction, where it indicates the probabilistic distribution of each action in each state, action is a dictionary, which can be derived using model and policy, the model is a dictionary mapping ${<state, action>}$ to next state which is written as ${P[s'|S_t = s, A_t = a]}$\n",
    "\n",
    "* There are other ways to represent the object, in accordance with the MP and MRP, the class can be written as (part of the implementation referred to MDP-DP-RL by Ashwin Rao):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "from typing import Mapping, Set, Tuple, Generic, Any, Callable\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "X = TypeVar('X')\n",
    "Y = TypeVar('Y')\n",
    "Z = TypeVar('Z')\n",
    "\n",
    "class MDP(Generic[S, A]):\n",
    "    \n",
    "    def map_reconstruct(self, d: Mapping[X, Tuple[Y, Z]])\\\n",
    "        -> Tuple[Mapping[X, Y], Mapping[X, Z]]:\n",
    "        d1 = {k: v1 for k, (v1, _) in d.items()}\n",
    "        d2 = {k: v2 for k, (_, v2) in d.items()}\n",
    "        return d1, d2\n",
    "    \n",
    "    def get_all_states(self, S):\n",
    "        # return the names of each state in a dictionary\n",
    "        return set(S.keys())\n",
    "    \n",
    "    def get_actions_set(self, mdp_data: Mapping[S, Mapping[A, Any]])\\\n",
    "        -> Mapping[S, Set[A]]:\n",
    "        actions_set = {}\n",
    "        for k, v in mdp_data.items():\n",
    "            actions_set[k] = set(v.keys())\n",
    "        return actiosn_set\n",
    "    \n",
    "    def get_actions_for_states(self, mdp_data: Mapping[S, Mapping[A, Any]])\\\n",
    "        -> Mapping[S, Set[A]]:\n",
    "        return {k: set(v.keys()) for k, v in mdp_data.items()}\n",
    "    \n",
    "    def convert_to_MRP_transition(self, d1, d2):\n",
    "        \"\"\"\n",
    "        The convert_to_MRP_transition should be a part of MDP class, where it takes two dictionary\n",
    "        It will return a dictionary mapping from state -> {state: probability},\n",
    "        which is the transition matrix for the MArkov Process\n",
    "        \"\"\"\n",
    "        data = {k: {v : 0 for v in self.all_states} for k in self.all_states}\n",
    "        for curt_state, mapping_s_a in d1.items():\n",
    "            for action, tup in mapping_s_a.items():\n",
    "                for next_state, probability in enumerate(tup):\n",
    "                    data[curt_state][next_state] += probability * d2[curt_state][action]\n",
    "        return data\n",
    "    \n",
    "    def convert_to_MRP_reward(self, d1, d2):\n",
    "        \"\"\"\n",
    "        The convert_to_MRP_reward should be a part of the MDP class, where it takes two dictionary, namely d1, and d2\n",
    "        \"\"\"\n",
    "        # where d1 is the mapping from state -> {action: (state, probability)\n",
    "        # d2 is the mapping from the state -> {action : reward}\n",
    "\n",
    "        # initialize the MRP with {state: {state: reward}} and the initial reward are all 0\n",
    "        data = {k: {v : 0 for v in self.all_states} for k in self.all_states}\n",
    "        for curt_state, mapping_s_a in d1.items():\n",
    "            for action, tup in mapping_s_a.items():\n",
    "                for next_state, probability in enumerate(tup):\n",
    "                    data[curt_state][next_state] += probability * d2[curt_state][action]\n",
    "\n",
    "        # another way to improve efficiency is to construct a 2D big matrix (state, state, probability) \n",
    "        # --- SEE MarkovDecisionProcess.convert_to_MRP()\n",
    "        return data\n",
    "\n",
    "    def __init__(self, info: Mapping[S, Mapping[A, Tuple[Mapping[S, float], float]]], gamma: float):\n",
    "        \n",
    "        # in this case, the params passes in into the init function is a big dictionary\n",
    "        # the data structure can be described as {state:{action:({state:probability, reward})}\n",
    "        \n",
    "        d = {k: self.map_reconstruct(v) for k, v in info.items()}\n",
    "        # after reconstructing the map, what we have here is a mapping from State -> {action: {state: probability}}, {action: reward}\n",
    "        d1, d2 = self.map_reconstruct(d)\n",
    "        # after reconstructing the map, what we have so far two dictionaries:\n",
    "        # where d1 is a mapping from state -> {action: {state: probabilities}}\n",
    "        # and d2 is a mapping from state -> {action, reward}\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        self.all_states: Set[S] = self.get_all_states(info)\n",
    "        \n",
    "        # state_action_dict is a mapping from a state to a set, which is consist of all the actions it can make at current state\n",
    "        self.state_action_dict: Mapping[S, Set[A]] = \\\n",
    "            self.get_actions_for_states(info)\n",
    "        \n",
    "        \n",
    "        self.transitions: Mapping[S, Mapping[A, Mapping[S, float]]] = d1\n",
    "        \n",
    "        self.rewards: Mapping[S, Mapping[A, float]] = d2\n",
    "        self.gamma: float = gamma\n",
    "        self.terminal_states: Set[S] = self.get_terminal_states()\n",
    "\n",
    "    def get_sink_states(self) -> Set[S]:\n",
    "        return {k for k, v in self.transitions.items() if\n",
    "                all(len(v1) == 1 and k in v1.keys() for _, v1 in v.items())\n",
    "                }\n",
    "\n",
    "    def get_terminal_states(self) -> Set[S]:\n",
    "\n",
    "        sink = self.get_sink_states()\n",
    "        return {s for s in sink if\n",
    "                all(is_approx_eq(r, 0.0) for _, r in self.rewards[s].items())}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "Separately implement the r(s,s',a) and R(s,a) = \\sum_{s'} p(s,s',a) * r(s,s',a) definitions of MDP. Write code to convert/cast the r(s,s',a) definition of MDP to the R(s,a) definition of MDP (put some thought into code design here)\n",
    "\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy recursive relationships. For any policy and any state s, the following consistency condition holds between the value of s and the value of its possible successor states:\n",
    "$${v_\\pi(s) = E[G_t | S_t = s] = \\Sigma_a \\pi(a|s)\\Sigma_{s'}\\Sigma_r p(s', r|s, a)[r + \\gamma E_\\pi[G_{t+1}|S_{t+1}=s']]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_MRP_reward(self, d1, d2):\n",
    "    \"\"\"\n",
    "    The convert_to_MRP_reward should be a part of the MDP class, where it takes two dictionary, namely d1, and d2\n",
    "    \"\"\"\n",
    "    # where d1 is the mapping from state -> {action: (state, probability)\n",
    "    # d2 is the mapping from the state -> {action : reward}\n",
    "    \n",
    "    # initialize the MRP with {state: {state: reward}} and the initial reward are all 0\n",
    "    data = {k: {v : 0 for v in self.all_states} for k in self.all_states}\n",
    "    for curt_state, mapping_s_a in d1.items():\n",
    "        for action, tup in mapping_s_a.items():\n",
    "            for next_state, probability in enumerate(tup):\n",
    "                data[curt_state][next_state] += probability * d2[curt_state][action]\n",
    "    \n",
    "    # another way to improve efficiency is to construct a 2D big matrix (state, state, probability) \n",
    "    # --- SEE MarkovDecisionProcess.convert_to_MRP()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "Write code to create a MRP given a MDP and a Policy\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MarkovRewardProcess class takes in a dictionary of transition matrix and reward matrix\n",
    "# The reward matrix can be constructed by the convert_to_MRP_reward method\n",
    "# a similar method is required to convert a MDP into transition matrix\n",
    "\n",
    "def convert_to_MRP_transition(self, d1, d2):\n",
    "    \"\"\"\n",
    "    The convert_to_MRP_transition should be a part of MDP class, where it takes two dictionary\n",
    "    It will return a dictionary mapping from state -> {state: probability},\n",
    "    which is the transition matrix for the MArkov Process\n",
    "    \"\"\"\n",
    "    data = {k: {v : 0 for v in self.all_states} for k in self.all_states}\n",
    "    for curt_state, mapping_s_a in d1.items():\n",
    "        for action, tup in mapping_s_a.items():\n",
    "            for next_state, probability in enumerate(tup):\n",
    "                data[curt_state][next_state] += probability * d2[curt_state][action]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a MRP object can be instatiated given transition and reward objects\n",
    "from MRP import MarkovRewardProcess\n",
    "\n",
    "# sudo object\n",
    "mdp_object = MDP({}, 0.95)\n",
    "transition = mdp_object.convert_to_MRP_transition(mdp_object.d1, mdp_object.d2)\n",
    "reward = mdp_object.convert_to_MRP_transition(mdp_object.d1, mdp_object.d2)\n",
    "mrp_object = MarkovRewardProcess(transition, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Write out all 8 MDP Bellman Equations and also the transformation from Optimal Action-Value function to Optimal Policy (in LaTeX)\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Optimality Equation for v*\n",
    "$${v_*(s) = max_a q_*(s, a)}$$\n",
    "Bellman Optimality Equation for q*\n",
    "$${q_*(s, a) = R_s^a + \\gamma \\Sigma_{s' \\in S}P_{ss'}^a(v_*(s'))}$$\n",
    "\n",
    "Bellman Optimality Equation for v*(2)\n",
    "$${v_*(s) = max_a R_s^a + \\gamma \\Sigma_{s' \\in S}P_{ss'}^a(v_*(s'))}$$\n",
    "\n",
    "Bellman Optimality Equation for q*(2)\n",
    "$${q_*(s, a) = R_s^a + \\gamma \\Sigma_{s' \\in S}P_{ss'}^a(max_a q_*(s, a))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Expectation Equation:  \n",
    "$${v_{\\pi}(s) = E_\\pi[R_{t+1} + \\gamma*v(s_{t+1})|S_t = s]}$$  \n",
    "The action-value function can be decomposed: \n",
    "$${q(s, a) = E_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] }$$  \n",
    "Bellman Expectation equation for ${V^\\pi}$\n",
    "$${v_\\pi(s) = \\Sigma_{a \\in A}\\pi(a|s)q_{\\pi}(s, a)}$$\n",
    "\n",
    "Bellman expectation Equation for Q*\n",
    "$${q(s, a) = R_s^a + \\gamma \\Sigma_{s' \\in S}P_{ss'}^a(v_{\\pi}(s'))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
