{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eig\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Write Proof (with precise notation) of the Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\nabla_{\\theta}J(\\theta) = \\int_{S}\\rho^{\\pi}(s)\\int_{A}\\nabla_{\\theta}\\pi(s, a, \\theta)Q^{\\pi}(s, a)da ds }$$\n",
    "$${J(\\theta) = \\int_s p_0(s_0)V^{\\pi}(s_0)ds_0 = \\int_s p_0(s_0)\\int_A\\pi(s_0, a_0, \\theta)Q^\\pi(s_0, a_0)da_0ds_0}$$\n",
    "$${\\nabla_{\\theta}J(\\theta) = \\int_s p_0(s_0)\\int_A\\nabla_\\theta\\pi(s_0, a_0, \\theta)Q^\\pi(s_0, a_0)da_0ds_0 + \\int_s p_0(s_0)\\int_A\\pi(s_0, a_0, \\theta)\\nabla_\\theta Q^\\pi(s_0, a_0)da_0ds_0}$$\n",
    "Now expand ${Q^\\pi(s_0, a_0)}$ using Bellman Equation:\n",
    "$${\\nabla_{\\theta}J(\\theta) = \\int_s p_0(s_0)\\int_A\\nabla_\\theta\\pi(s_0, a_0, \\theta)Q^\\pi(s_0, a_0)da_0ds_0 + \\int_s p_0(s_0)\\int_A\\pi(s_0, a_0, \\theta)\\nabla_\\theta (R_{s_0}^{a_0} + \\int_s\\gamma*P_{s_0, s_1}^{a_0}V^\\pi(s_1)d(s_1))da_0ds_0}$$\n",
    "Note ${\\nabla_\\theta R_{s_0}^{a_0} = 0}$, and remove it from the expression\n",
    "$${\\nabla_{\\theta}J(\\theta) = \\int_s p_0(s_0)\\int_A\\nabla_\\theta\\pi(s_0, a_0, \\theta)Q^\\pi(s_0, a_0)da_0ds_0 + \\int_s p_0(s_0)\\int_A\\pi(s_0, a_0, \\theta)\\nabla_\\theta (\\int_s\\gamma*P_{s_0, s_1}^{a_0}V^\\pi(s_1)d(s_1))da_0ds_0}$$\n",
    "Now bring the ${\\nabla_\\theta}$ inside the integral and apply only on ${V^\\pi(s_1)}$\n",
    "$${\\nabla_{\\theta}J(\\theta) = \\int_s p_0(s_0)\\int_A\\nabla_\\theta\\pi(s_0, a_0, \\theta)Q^\\pi(s_0, a_0)da_0ds_0 + \\int_s p_0(s_0)\\int_A\\pi(s_0, a_0, \\theta)(\\int_s\\gamma*P_{s_0, s_1}^{a_0}\\nabla_\\theta V^\\pi(s_1)d(s_1))da_0ds_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Derive the score function for softmax policy (for finite set of actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(s,a|\\theta) = \\frac{\\exp(\\theta^T \\phi(s,a))}{\\sum_b \\exp(\\theta^T \\phi(s,b))}$$\n",
    "$$\\nabla_{\\theta} \\log \\pi(s,a|\\theta) = \\frac{\\sum_b \\exp(\\theta^T \\phi(s,b))}{\\exp(\\theta^T \\phi(s,a))}*\\frac{\\exp(\\theta^T \\phi(s,a))\\phi(s,a)*\\sum_b \\exp(\\theta^T \\phi(s,b)) -\\sum_b \\exp(\\theta^T \\phi(s,b))*\\phi(s,b)*\\exp(\\theta^T \\phi(s,a))}{\\sum_b \\exp(\\theta^T \\phi(s,b))^2}$$\n",
    "$$\\nabla_{\\theta} \\log \\pi(s,a|\\theta) = \\phi(s,a) - \\sum_b \\theta^T \\phi(s,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Derive the score function for gaussian policy (for continuous actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(s,a|\\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp(\\frac{a - \\theta^T \\phi(s)}{-2\\sigma^2})$$\n",
    "$$\\nabla_{\\theta} \\log \\pi(s,a|\\theta) =\\frac{(a - \\theta^T \\phi(s))\\phi(s)}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Write code for the REINFORCE Algoithm (Monte-Carlo Policy Gradient Algorithm, i.e., no Critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update $\\theta$ by stochastic gradient ascend using PGT  \n",
    "Using $G_t = \\sum_{K = t}^{T}\\gamma^{k-t}r_k$ as an unbiased sample of $Q^\\pi(s_t, a_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce:\n",
    "    # the episode is a list of tuples (state:state, action:action, reward:reward)\n",
    "    def __init__(self, episode, hidden_size = 5, gamma = 1, learning_rate = 1e-3, actions = 5):\n",
    "        self.episode = episode\n",
    "        self.hidden_size = hidden_size # representation of the length of theta\n",
    "        self.gamma = gamma\n",
    "        self.alpha = learning_rate\n",
    "        self.actions = actions # the number of actions\n",
    "        self.theta = np.random.random_sample((self.hidden_size, 1))\n",
    "        self.features = np.random.ramdom_sample(self.actions, self.hidden_size)\n",
    "        \n",
    "    def feature(self, state, action):\n",
    "        return self.features[self.get_number_action(action), self.get_number_state(state)]\n",
    "    \n",
    "    def expectation(self, state):\n",
    "        expectation = 0\n",
    "        for action in range(self.actions):\n",
    "            expectation += self.feature(state, action)\n",
    "        return expectation / self.actions\n",
    "    \n",
    "    def score(self, state, action):\n",
    "        return self.feature(state, action) - self.expectation(state)\n",
    "    \n",
    "    def get_G(self, step):\n",
    "        current_reward = 0\n",
    "        p = 1\n",
    "        for i in range(len(self.episode) - step):\n",
    "            current_reward += self.gamma * p * self.episode[i].reward\n",
    "            p *= self.gamma\n",
    "    \n",
    "    def forward(self):\n",
    "        for t in range(len(self.episode)):\n",
    "            g = self.get_G(t) \n",
    "            self.theta = self.alpha * np.pow(self.gamma, t) * \\\n",
    "            self.score(self.episode[t].state, self.episode[t].action) * g\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "Write Proof (with proper notation) of the Compatible Function Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following two conditions are satisfied:  \n",
    "critic gradient is compatible with the Actor score function:\n",
    "$$\\nabla_{\\theta} \\log \\pi(s,a|\\theta) = \\nabla_{w} Q(s,a,w)$$  \n",
    "critic parameters w minimize the following mean-squared error:  \n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) (Q_{\\pi}(s,a) - Q(s,a,w))^2 da ds$$\n",
    "Then the Policy Gradient using critic $Q(s,a,w)$ is exact:  \n",
    "$$\\nabla_{\\theta} J(\\theta) \\int_s \\rho_{\\pi}(s) \\int_a \\nabla_{\\theta} \\pi(s,a,\\theta) Q(s,a,w)$$\n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) (Q_{\\pi}(s,a) - Q(s,a,w)) \\nabla_{w} Q(s,a,w) da ds = 0$$\n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) (Q_{\\pi}(s,a) - Q(s,a,w)) \\nabla_{\\theta} \\log \\pi(s,a;\\theta) da ds = 0$$\n",
    "$$ \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) Q_{\\pi}(s,a) \\nabla_{\\theta} \\log \\pi(s,a;\\theta) da ds = \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) Q(s,a,w) \\nabla_{\\theta} \\log \\pi(s,a;\\theta) da ds$$\n",
    "$$ \\nabla_{\\theta}J(\\theta) = \\int_s \\rho_{\\pi}(s) \\int_a \\pi(s,a,\\theta) Q_{\\pi}(s,a) \\nabla_{\\theta} \\log \\pi(s,a;\\theta) da ds $$\n",
    "$$ \\nabla_{\\theta}J(\\theta) = \\int_s \\rho_{\\pi}(s) \\int_a \\nabla_{\\theta} \\pi(s,a;\\theta) Q(s,a,w)da ds$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
