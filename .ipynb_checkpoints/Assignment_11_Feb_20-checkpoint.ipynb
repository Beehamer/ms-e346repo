{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Write code for the interface for RL algorithms with value function approximation. The core of this interface should be a function from a (state, action) pair to a sampling of the (next state, reward) pair. It is important that this interface doesn't present the state-transition probability model or the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import TypeVar,Mapping, Set, Generic, Sequence, Callable, Tuple, Dict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "X = TypeVar('X')\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy approximation for Pi function\n",
    "class Pi_NN(nn.Module):\n",
    "    def __init__(self, input_size, state_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, state_size, bias = True)\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# Function approximation for the Q function     \n",
    "class Q_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1, bias = True)\n",
    "        )\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        return out\n",
    "\n",
    "# Function approximation for V function    \n",
    "class V_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.approximator = nn.Sequential(\n",
    "            nn.Linear(input_size, 2 * hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden_size, hidden_size, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1, bias = True)\n",
    "        )\n",
    "    def forward(self, feature):\n",
    "        out = self.approximator(feature)\n",
    "        return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"\n",
    "    The stochasic policy that maps a state to the transition probability\n",
    "    The policy is a mapping from a state to an action, i.e. given a state, figuring out the probability of selecting some actions\n",
    "    \"\"\"\n",
    "    def __init__(self, data: Dict[S, Mapping[A, float]]) -> None:\n",
    "        self.data = data\n",
    "\n",
    "    def get_state_probabilities(self, state: S) -> Mapping[A, float]:\n",
    "        return self.data[state]\n",
    "\n",
    "    def get_state_action_probability(self, state: S, action: A) -> float:\n",
    "        return self.get_state_probabilities(state).get(action, 0.)\n",
    "\n",
    "    # update state/ action probability using epsilon greedy\n",
    "    def update_state_action_to_epsilon_greedy(self, state: S, action_value_dict: Mapping[A, float], epsilon: float):\n",
    "        max_act = max(action_value_dict.items(), key=itemgetter(1))[0]\n",
    "        if epsilon == 0:\n",
    "            ret = {max_act: 1.}\n",
    "        else:\n",
    "            ret = {a: epsilon / len(action_value_dict) + (1. - epsilon if a == max_act else 0.) for a in action_value_dict.keys()}\n",
    "        self.policy_data[state] = ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.policy_data.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.policy_data.__str__()\n",
    "\n",
    "class RLFuncApproxBase:\n",
    "\n",
    "    NUM_SAMPLES_PER_ACTION = 10\n",
    "\n",
    "    def __init__(self, mdp_rep_for_rl, epsilon: float, num_episodes: int, max_steps: int, fa_spec):\n",
    "\n",
    "        self.mdp_rep: MDPRepForRLFA = mdp_rep_for_rl\n",
    "        \n",
    "        # TODO: epsilon with decay should be allowed\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes: int = num_episodes\n",
    "        self.max_steps: int = max_steps\n",
    "        self.vf_fa: FuncApproxBase = V_NN(len(self.mdp_rep.states))\n",
    "        self.qvf_fa: FuncApproxBase = Q_NN(len(self.mdp_rep.states))\n",
    "        # the simulator of getting the next, given the current state and action\n",
    "        self.state_action_func = self.mdp_rep.state_action_func\n",
    "\n",
    "    def get_init_policy_func(self):\n",
    "        return Policy({s: {a: 1. / len(v) for a in v} for s, v in self.state_action_dict.items()})\n",
    "\n",
    "    def get_value_func_fa(self, polf):\n",
    "        qv_func = self.vf_fa # the function approximation of the reward in each state\n",
    "        # return the expectation of current states\n",
    "        return sum(polf(s)[a] * qv_func(s)(a) for a in self.state_action_func(s))\n",
    "\n",
    "\n",
    "    def get_value_func(self, pol_func):\n",
    "        return self.get_value_func_fa(lambda s, pol_func=pol_func: get_pdf_from_samples(pol_func(s)(len(self.state_action_func(s)) * RLFuncApproxBase.NUM_SAMPLES_PER_ACTION)))\n",
    "\n",
    "\n",
    "    def get_act_value_func(self, pol_func):\n",
    "        # a weighted averagex\n",
    "        return self.get_qv_func_fa(\n",
    "            lambda s, pol_func=pol_func: get_pdf_from_samples(\n",
    "                pol_func(s)(len(self.state_action_func(s)) *\n",
    "                            RLFuncApproxBase.NUM_SAMPLES_PER_ACTION)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_optimal_det_policy_func(self):\n",
    "        qv_func = self.get_qv_func_fa(None)\n",
    "\n",
    "        # noinspection PyShadowingNames\n",
    "        def detp_func(s: S, qv_func=qv_func) -> A:\n",
    "            return max(\n",
    "                [(a, qv_func(s)(a)) for a in self.state_action_func(s)],\n",
    "                key=itemgetter(1)\n",
    "            )[0]\n",
    "\n",
    "        return detp_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functon to train the neural network to converge to the value provided\n",
    "\n",
    "def train(model, data, value, number_epochs, learning_rate = 1e-3):\n",
    "    # the data passed in should be a torch tensor\n",
    "    cross_entrophy_loss = nn.CrossEntropyLoss(reduce=True)\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    # define the loss of the model\n",
    "    num_of_samples, _ = data.size()\n",
    "    for epc in range(number_epochs):\n",
    "        # use batch size of 1 for brevity\n",
    "        for i in range(num_of_samples):\n",
    "            sample = data[i, :] # the current sample used to train the model\n",
    "            y_hat = model(sampel)\n",
    "            y = value[i]\n",
    "            loss = cross_entrophy_loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    torch.save(model.state_dict(), os.path.join('best_model', 'model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the function approximation, linear value function approximation could be used and the update is\n",
    "$${\\nabla_w\\hat{v}(S,w) = x(S)}$$\n",
    "$${\\Delta w = \\alpha(v_\\pi(S) - \\hat{v}(S, w))x(S)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "Implement any Monte-Carlo Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return G_t is unbiased, noisy sample of true value v_pi(S_t)\n",
    "def MCwithFA(state_list, value_list, hidden_size, alpha = 1e-3, gamma = 1, init_w = None):\n",
    "    # hidden_size the feature representation of a state\n",
    "    if len(state_list) != len(value_list):\n",
    "        raise ValueError('The number of the states and the number of values should be the same')\n",
    "    if init_w is not None:\n",
    "        w = init_w\n",
    "    else:\n",
    "        w = np.random.rand(hidden_size)\n",
    "    steps = len(state_list) # total steps in one episode\n",
    "    for step in range(steps):\n",
    "        current_state = state_list[step]\n",
    "        current_value = value_list[step]\n",
    "        v_hat = np.dot(w, current_state)\n",
    "        dw = alpha * (current_value - v_hat)*current_state\n",
    "        w = w - dw\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42031383 0.32683631 0.02448432 0.90036345]\n"
     ]
    }
   ],
   "source": [
    "# an example of a very simple classifier\n",
    "# where the state is represented as a vector of features\n",
    "a = np.array([1, 1, 1, 1])\n",
    "b = np.array([0, 0, 0, 0])\n",
    "state_list = [a, b]\n",
    "value_list = [1, 0]\n",
    "\n",
    "w = MCwithFA(state_list, value_list, 4)\n",
    "print (w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Implement 1-step TD Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly we are using linear functio approximation to estimate the value of the state\n",
    "def TD0withFA(state_list, value_list, hidden_size, alpha = 1e-3, gamma = 1, init_w = None):\n",
    "    # hidden_size the feature representation of a state\n",
    "    if len(state_list) != len(value_list):\n",
    "        raise ValueError('The number of the states and the number of values should be the same')\n",
    "    if init_w is not None:\n",
    "        w = init_w\n",
    "    else:\n",
    "        w = np.random.rand(hidden_size)\n",
    "    steps = len(state_list - 1) # total steps in one episode\n",
    "    for step in range(steps):\n",
    "        current_state = state_list[step]\n",
    "        current_value = value_list[step] + np.dot(w, state_list[step + 1]) # R + one-step looking ahead\n",
    "        v_hat = np.dot(w, current_state)\n",
    "        dw = alpha * (current_value - v_hat)*current_state\n",
    "        w = w - dw\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82438798 0.97422517 1.32078388 0.96470432]\n"
     ]
    }
   ],
   "source": [
    "# an example using TD(0) and linear function approximation to estimate the value function of each state\n",
    "a = np.array([1, 1, 1, 1])\n",
    "b = np.array([0, 0, 0, 0])\n",
    "c = np.array([1, 1, 1, 0])\n",
    "d = np.array([1, 1, 0, 0])\n",
    "e = np.array([1, 0, 0, 0])\n",
    "state_list = [a, b, c, d, e]\n",
    "value_list = [1, 0, 0, 0, 0]\n",
    "\n",
    "w = MCwithFA(state_list, value_list, 4, alpha = 1e-1)\n",
    "print (w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Implement Eligibility-Traces-based TD(lambda) Prediction algorithm with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward view TD(lambda) algorithm with eligibility trace\n",
    "# similarly we are using linear functio approximation to estimate the value of the state\n",
    "def getGLambd(value_list, alpha, lambd, gamma, value = None):\n",
    "    if value is None:\n",
    "        value = np.zeros(len(value_list))\n",
    "    # the lambd return combines all n-step returns\n",
    "    # defining n step returns\n",
    "    G = []\n",
    "    for i in range(len(value_list)):\n",
    "        # for each i, calculating the n step return separately as a list\n",
    "        G_t = []\n",
    "        for j in range(i, len(value_list)):\n",
    "            # defining the edge cases\n",
    "            if j == 0:\n",
    "                g_t_j = value_list[j] + gamma * value[j + 1]\n",
    "            if j == len(value_list) - 1:\n",
    "                g_t_j = G_t[-1]\n",
    "            g_t_j = G_t[-1] + gamma * value[j + 1]\n",
    "            G_t.append(g_t_j)\n",
    "        G.append(G_t)\n",
    "    # G is the the value of each state, and each value of G is a list, having all the G_t_n values\n",
    "    return G\n",
    "    \n",
    "# the lambda return is also a biased sample of true value\n",
    "def TDlambdawithFA(state_list, value_list, hidden_size, alpha = 1e-3, lambd = 0.7, gamma = 1, init_w = None):\n",
    "    # hidden_size the feature representation of a state\n",
    "    \n",
    "    # getting G_t_n\n",
    "    G = getGLambd(value_list, alpha, lambd, gamma)\n",
    "    \n",
    "    # getting G_t_lambd\n",
    "    \n",
    "    G_t_lambd = []\n",
    "    for step in range(len(G)):\n",
    "        p = 1 # indicator variable to calculating the lambd to the power of n\n",
    "        current_n_value = G[step]\n",
    "        current_G_t_lambd = 0\n",
    "        for g_value in current_n_value:\n",
    "            current_G_t_lambd += p * g_value\n",
    "            p *= lambd\n",
    "        # the final result should have a factor of 1 - lambd\n",
    "        G_t_lambd.append((1 - lambd) * current_G_t_lambd)\n",
    "        \n",
    "    # using the G_t_lambd as the value function to update the value of each state\n",
    "    if len(state_list) != len(value_list):\n",
    "        raise ValueError('The number of the states and the number of values should be the same')\n",
    "    if init_w is not None:\n",
    "        w = init_w\n",
    "    else:\n",
    "        w = np.random.rand(hidden_size)\n",
    "    steps = len(state_list - 1) # total steps in one episode\n",
    "    \n",
    "    # constructing the E matrix with the number of states and the number of features\n",
    "    E_t = np.zeros(steps, hidden_size)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        current_state = state_list[step]\n",
    "        current_value = G_t_lambd[step] + np.dot(w, state_list[step + 1]) # R + one-step looking ahead\n",
    "        v_hat = np.dot(w, current_state)\n",
    "        delta_t = alpha * (current_value - v_hat)*current_state\n",
    "        E = gamma * lambd * E\n",
    "        E[step] += current_state\n",
    "        dw = alpha * delta_t * E[step]\n",
    "        w = w - dw\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "Implement SARSA and SARSA(Lambda) with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some code from last assignment\n",
    "\n",
    "\n",
    "def random_generator_based_on_prob(prob_dict: Mapping[S, float]) -> Callable[[], S]:\n",
    "    outcomes, probabilities = zip(*prob_dict.items())\n",
    "    rvd = rv_discrete(values=(range(len(outcomes)), probabilities))\n",
    "\n",
    "    return lambda rvd=rvd, outcomes=outcomes: outcomes[rvd.rvs(size=1)[0]]\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"\n",
    "    The MDP is defined to initalize with a state_action_dict and state_reward_dict\n",
    "    The MDP is conpatible with the requirement for Monte Carlo or TD learning algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self,state_action_dict: Mapping[S, Set[A]], terminal_states: Set[S], state_reward_gen_dict, gamma: float) -> None:\n",
    "        \n",
    "        # a mapping from a tuple of (state, action) to the reward\n",
    "        self.state_action_dict: Mapping[S, Set[A]] = state_action_dict\n",
    "        \n",
    "        self.terminal_states: Set[S] = terminal_states\n",
    "        self.state_reward_gen_dict: Type1 = state_reward_gen_dict\n",
    "        self.state_action_func = lambda x: self.state_action_dict[x],\n",
    "        self.gamma=gamma,\n",
    "        self.terminal_state_func = lambda x: x in self.terminal_states,\n",
    "        self.state_reward_gen_func = lambda x, y: self.state_reward_gen_dict[x][y](),\n",
    "        \n",
    "        # initialize the state generator with equal probability of each state \n",
    "        self.init_state_generator = random_generator_based_pn_prob(\n",
    "                {s: 1. / len(self.state_action_dict) for s\n",
    "                 in self.state_action_dict.keys()}\n",
    "            ),\n",
    "        \n",
    "        # initialize the (state, action) generator with equal probability of each pair\n",
    "        self.init_state_action_generator = random_generator_based_on_prob(\n",
    "                {(s, a): 1. / sum(len(v) for v\n",
    "                                  in self.state_action_dict.values())\n",
    "                 for s, v1 in self.state_action_dict.items() for a in v1}\n",
    "        )\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Mapping, Set, Tuple, Sequence, Any, Callable, TypeVar, Dict\n",
    "\n",
    "def epsilon_greedy(action_value_dict, epsilon: float) -> Mapping[A, float]:\n",
    "    \"\"\"\n",
    "    Using Epsilon-Greedy method to select the action based on the value\n",
    "    @return: the function returns the a dictionary of a action and its probability to be selected\n",
    "    \"\"\"\n",
    "    max_act = max(action_value_dict.items(), key=itemgetter(1))[0]\n",
    "    m = len(action_value_dict)\n",
    "    if epsilon == 0:\n",
    "        return {max_act: 1.}\n",
    "    else:\n",
    "        # with 1 / epsilon to select a random  \n",
    "        return {action: epsilon / m + (1. - epsilon if a == max_act else 0.) for action in action_value_dict.keys()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, data: Dict[S, Mapping[A, float]]) -> None:\n",
    "        self.policy_data = data\n",
    "\n",
    "    def get_state_probabilities(self, state: S) -> Mapping[A, float]:\n",
    "        return self.policy_data[state]\n",
    "    \n",
    "    def update_state_action_to_greedy(self, state, action_value_dict):\n",
    "        self.policy_data[state] = {max(action_value_dict.item(), key=itemgetter(1))[0]:1.0}\n",
    "\n",
    "    def get_state_action_probability(self, state: S, action: A) -> float:\n",
    "        return self.get_state_probabilities(state).get(action, 0.)\n",
    "\n",
    "    def edit_state_action_to_epsilon_greedy(\n",
    "        self,\n",
    "        state: S,\n",
    "        action_value_dict: Mapping[A, float],\n",
    "        epsilon: float\n",
    "    ) -> None:\n",
    "        self.policy_data[state] = self.epsilon_greedy(action_value_dict, epsilon)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.policy_data.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.policy_data.__str__()\n",
    "\n",
    "\n",
    "class SARSA:\n",
    "\n",
    "    def __init__(self, mdp: MDP, epsilon: float, learning_rate: float, learning_rate_decay: float, \\\n",
    "                 lambd: float, num_episodes: int, max_steps: int, initial_policy: Policy):\n",
    "\n",
    "        self.mdp = mdp,\n",
    "        self.lambd = lambd\n",
    "        self.epsilon = epsilon,\n",
    "        self.num_episodes = num_episodes,\n",
    "        self.max_steps = max_steps\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.gamma_lambda = self.mdp.gamma * lambd\n",
    "        \n",
    "        self.policy = initial_policy\n",
    "    \n",
    "    def epsilon_greedy(self, policy, current_state, state_value_dict, epsilon):\n",
    "        # the probability of selecting certain action based on the policy\n",
    "        actions = policy.get_state_probabilities(current_state)\n",
    "        \n",
    "        # the action chosen based on greedy algorithm\n",
    "        max_act = current_state\n",
    "        for action in actions:\n",
    "            if state_value_dict[current_state][action] > state_value_dict[current_state][max_act]:\n",
    "                max_act = action\n",
    "        \n",
    "        m = len(action_value_dict)\n",
    "        policy.edit_state_action_to_epsilon_greedy(current_state, self.mdp.action_value_dict)\n",
    "        if epsilon == 0:\n",
    "            return {max_act: 1.}\n",
    "        else:\n",
    "            # with 1 / epsilon to select a random  \n",
    "            return {action: epsilon / m + (1. - epsilon if a == max_act else 0.) for action in action_value_dict.keys()}\n",
    "        \n",
    "        \n",
    "    def update_one_episode(self, current_state, current_action, state_action_value_dict, policy):\n",
    "        # before updating using SARSA\n",
    "        curt_state = current_state\n",
    "        online_state_value_dict = copy.deepcopy(state_action_value_dict)\n",
    "        curt_action = current_action\n",
    "        for step in range(self.max_steps):\n",
    "            # generate next step based on epsilon-greedy\n",
    "            actions = self.epsilon_greedy(policy, current_state, online_state_value_dict, self.epsilon)\n",
    "            # here the next action to take is based on the probability generated by the epsilon greedy\n",
    "            next_action_generator = random_generator_based_on_prob(actions)\n",
    "            action_to_take = next_action_generator()\n",
    "            \n",
    "            next_state, reward = self.mdp.state_reward_gen_dict[current_state][curt_action]()\n",
    "            \n",
    "            # online update for Q(S,A)\n",
    "            online_state_value_dict[curt_state][curt_action] = online_state_value_dict[curt_state][curt_action] + \\\n",
    "            self.learning_rate * (reward + self.gamma * online_state_value_dict[next_state][action_to_take] - online_state_value_dict[curt_state][curt_action])\n",
    "            if curt_state in self.mdp.terminal_states:\n",
    "                break\n",
    "            else:\n",
    "                curt_state = next_state\n",
    "                curt_action = action_to_take\n",
    "        # update the policy and the Q(S,A) accordingly\n",
    "        return (policy, online_state_value_dict)\n",
    "    \n",
    "    def SARSA_update(self, pol:Policy, inital_state, intial_action):\n",
    "        \"\"\"\n",
    "        Update the policy and Q(S,A) using SARSA online update\n",
    "        \"\"\"\n",
    "        \n",
    "        state_action_value_dict = self.mdp.state_action_dict\n",
    "        \n",
    "\n",
    "        for episode in range(self.max_episodes):\n",
    "            current_state = initial_state\n",
    "            current_action = initial_action\n",
    "            (policy, state_action_value_dict) = self.update_one_episode(current_state, \\\n",
    "                                                                        current_action, \\\n",
    "                                                                        copy.deepcopy(state_action_value_dict), \\\n",
    "                                                                        copy.deepcopy(policy))\n",
    "            \n",
    "\n",
    "        return (policy, state_action_value_dict)\n",
    "    \n",
    "    \n",
    "    # a helper function used to calculate the G_t_lambd value for current state\n",
    "    def calculate_g_t_lambd(step, g_t_list, lambd):\n",
    "        total_step = len(g_t_list)\n",
    "        g_t_lambd = 0\n",
    "        current_coeff = 1\n",
    "        for i in range(step, total_step, 1):\n",
    "            g_t_lambd += current_coeff * g_t_list[i]\n",
    "            current_coeff *= lamba\n",
    "        return (1.0 - lambd) * g_t_lambd\n",
    "    \n",
    "    def get_one_TDLambda_update(self, pol: Policy, state_value_dict, action_generator_for_each_state):\n",
    "        # Update MDP by running one episode of TD-lambda learning\n",
    "        \n",
    "        \n",
    "        # preserve the state value/ action information of the current MDP\n",
    "        state_action_dict = self.mdp.state_action_dict\n",
    "        state_value = copy.deepcopy(state_value_dict)\n",
    "        \n",
    "        # generate the trace of this episode\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        value_list = []\n",
    "        \n",
    "        # use the initial state generate by MDP, it could also be initialized to a fixed state\n",
    "        current_state = self.mdp.init_state_generator()\n",
    "        \n",
    "        for step in range(self.max_steps):\n",
    "            action_to_take = action_generator_for_each_state[current_state]()\n",
    "            next_state, reward = self.mdp.state_reward_gen_dict[current_state][action_to_take]()\n",
    "            state_list.append(current_state)\n",
    "            action_list.append(action_to_take)\n",
    "            value_list.append(reward)\n",
    "            if current_state in self.mdp.terminal_states:\n",
    "                break\n",
    "            else:\n",
    "                # increment by one step\n",
    "                current_state = next_state\n",
    "        \n",
    "        g_t_list = []\n",
    "        # calculating G_t in a backward manner\n",
    "        for i in range(len(state_list) - 1, -1. -1):\n",
    "            if i == len(state_list) - 1:\n",
    "                current_g_t = value_list[i]\n",
    "            else:\n",
    "                current_g_t = value_list[i] + g_t_list[-1] * self.lambd\n",
    "            g_t_list.append(current_g_t)\n",
    "        # reverse the g_t_list to match the time\n",
    "        g_t_list = g_t_list[::-1]\n",
    "        \n",
    "        #  update the value function \n",
    "        for step in range(len(state_list)):\n",
    "            # this time only consider the state visited in this episode\n",
    "            g_t_lambd = calculate_g_t_lambd(step, g_t_list, lambd)\n",
    "            state_value[state_list[step]] += self.learning_rate * (g_t_lambd - state_value[state_list[step]])\n",
    "        \n",
    "        return state_value\n",
    "    \n",
    "    # the update function running over all the episodes\n",
    "    def update(self, pol:Policy):\n",
    "        state_action_dict = self.mdp.state_action_dict\n",
    "        # initate the value for each state as 0\n",
    "        V_s = {s: 0. for s in sa_dict.keys()}\n",
    "        \n",
    "        # action generator for each state\n",
    "        action_generator_for_each_state = {s: random_generator_based_on_prob(pol.get_state_probabilities(s))\n",
    "                        for s in sa_dict.keys()}\n",
    "        episodes = 0\n",
    "        updates = 0\n",
    "\n",
    "        for episode in range(self.max_episodes):\n",
    "            et_dict = {s: 0. for s in sa_dict.keys()}\n",
    "            state = self.mdp_rep.init_state_gen()\n",
    "            steps = 0\n",
    "            terminate = False\n",
    "\n",
    "            V_s = self.get_one_TDLambda_update(pol, copy.deepcopy(V_s), action_generator_for_each_state)\n",
    "\n",
    "        return V_s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change state_value[state_list[step]] += self.learning_rate * (g_t_lambd - state_value[state_list[step]]) to\n",
    "# update w instead of state probability\n",
    "\n",
    "# the idea behind is very similar, but what is differnent is that we no longer change the state probability directly,\n",
    "# and instead we are using w to approximate each value appreared\n",
    "\n",
    "class SARSAwithFunctionApproximation:\n",
    "\n",
    "    def __init__(self, mdp: MDP, epsilon: float, learning_rate: float, learning_rate_decay: float, \\\n",
    "                 lambd: float, num_episodes: int, max_steps: int, initial_policy: Policy, hidden_size: int, w = None):\n",
    "\n",
    "        self.mdp = mdp,\n",
    "        self.lambd = lambd\n",
    "        self.epsilon = epsilon,\n",
    "        self.num_episodes = num_episodes,\n",
    "        self.max_steps = max_steps\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.gamma_lambda = self.mdp.gamma * lambd\n",
    "        \n",
    "        self.policy = initial_policy\n",
    "        \n",
    "        # initiate the w used to estimate the values\n",
    "        if w is None:\n",
    "            self.w = np.random.rand(hidden_size)\n",
    "        else:\n",
    "            self.w = w\n",
    "            \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def getGLambd(value_list, alpha, lambd, gamma, value = None):\n",
    "        if value is None:\n",
    "            value = np.zeros(len(value_list))\n",
    "        # the lambd return combines all n-step returns\n",
    "        # defining n step returns\n",
    "        G = []\n",
    "        for i in range(len(value_list)):\n",
    "            # for each i, calculating the n step return separately as a list\n",
    "            G_t = []\n",
    "            for j in range(i, len(value_list)):\n",
    "                # defining the edge cases\n",
    "                if j == 0:\n",
    "                    g_t_j = value_list[j] + gamma * value[j + 1]\n",
    "                if j == len(value_list) - 1:\n",
    "                    g_t_j = G_t[-1]\n",
    "                g_t_j = G_t[-1] + gamma * value[j + 1]\n",
    "                G_t.append(g_t_j)\n",
    "            G.append(G_t)\n",
    "        # G is the the value of each state, and each value of G is a list, having all the G_t_n values\n",
    "        return G\n",
    "\n",
    "    # the lambda return is also a biased sample of true value\n",
    "    def TDlambdawithFA(state_list, value_list, hidden_size, alpha = 1e-3, lambd = 0.7, gamma = 1, init_w = None):\n",
    "        # hidden_size the feature representation of a state\n",
    "\n",
    "        # getting G_t_n\n",
    "        G = getGLambd(value_list, alpha, lambd, gamma)\n",
    "\n",
    "        # getting G_t_lambd\n",
    "\n",
    "        G_t_lambd = []\n",
    "        for step in range(len(G)):\n",
    "            p = 1 # indicator variable to calculating the lambd to the power of n\n",
    "            current_n_value = G[step]\n",
    "            current_G_t_lambd = 0\n",
    "            for g_value in current_n_value:\n",
    "                current_G_t_lambd += p * g_value\n",
    "                p *= lambd\n",
    "            # the final result should have a factor of 1 - lambd\n",
    "            G_t_lambd.append((1 - lambd) * current_G_t_lambd)\n",
    "\n",
    "        # using the G_t_lambd as the value function to update the value of each state\n",
    "        if len(state_list) != len(value_list):\n",
    "            raise ValueError('The number of the states and the number of values should be the same')\n",
    "        if init_w is not None:\n",
    "            w = init_w\n",
    "        else:\n",
    "            w = np.random.rand(hidden_size)\n",
    "        steps = len(state_list - 1) # total steps in one episode\n",
    "\n",
    "        # constructing the E matrix with the number of states and the number of features\n",
    "        E_t = np.zeros(hidden_size)\n",
    "\n",
    "        for step in range(steps):\n",
    "            current_state = state_list[step]\n",
    "            current_value = G_t_lambd[step] + np.dot(w, state_list[step + 1]) # R + one-step looking ahead\n",
    "            v_hat = np.dot(w, current_state)\n",
    "            delta_t = alpha * (current_value - v_hat)*current_state\n",
    "            E = gamma * lambd * E\n",
    "            E += current_state\n",
    "            dw = alpha * delta_t * E\n",
    "            w = w - dw\n",
    "        return w\n",
    "    \n",
    "    def epsilon_greedy(self, policy, current_state, state_value_dict, epsilon):\n",
    "        # the probability of selecting certain action based on the policy\n",
    "        actions = policy.get_state_probabilities(current_state)\n",
    "        \n",
    "        # the action chosen based on greedy algorithm\n",
    "        max_act = current_state\n",
    "        for action in actions:\n",
    "            if state_value_dict[current_state][action] > state_value_dict[current_state][max_act]:\n",
    "                max_act = action\n",
    "        \n",
    "        m = len(action_value_dict)\n",
    "        policy.edit_state_action_to_epsilon_greedy(current_state, self.mdp.action_value_dict)\n",
    "        if epsilon == 0:\n",
    "            return {max_act: 1.}\n",
    "        else:\n",
    "            # with 1 / epsilon to select a random  \n",
    "            return {action: epsilon / m + (1. - epsilon if a == max_act else 0.) for action in action_value_dict.keys()}\n",
    "        \n",
    "        \n",
    "    def update_one_episode(self, current_state, current_action, state_action_value_dict, policy):\n",
    "        # before updating using SARSA\n",
    "        curt_state = current_state\n",
    "        online_state_value_dict = copy.deepcopy(state_action_value_dict)\n",
    "        curt_action = current_action\n",
    "        for step in range(self.max_steps):\n",
    "            # generate next step based on epsilon-greedy\n",
    "            actions = self.epsilon_greedy(policy, current_state, online_state_value_dict, self.epsilon)\n",
    "            # here the next action to take is based on the probability generated by the epsilon greedy\n",
    "            next_action_generator = random_generator_based_on_prob(actions)\n",
    "            action_to_take = next_action_generator()\n",
    "            \n",
    "            next_state, reward = self.mdp.state_reward_gen_dict[current_state][curt_action]()\n",
    "            \n",
    "            # online update for Q(S,A)\n",
    "            online_state_value_dict[curt_state][curt_action] = online_state_value_dict[curt_state][curt_action] + \\\n",
    "            self.learning_rate * (reward + self.gamma * online_state_value_dict[next_state][action_to_take] - online_state_value_dict[curt_state][curt_action])\n",
    "            if curt_state in self.mdp.terminal_states:\n",
    "                break\n",
    "            else:\n",
    "                curt_state = next_state\n",
    "                curt_action = action_to_take\n",
    "        # update the policy and the Q(S,A) accordingly\n",
    "        return (policy, online_state_value_dict)\n",
    "    \n",
    "    def SARSA_update(self, pol:Policy, inital_state, intial_action):\n",
    "        \"\"\"\n",
    "        Update the policy and Q(S,A) using SARSA online update\n",
    "        \"\"\"\n",
    "        \n",
    "        state_action_value_dict = self.mdp.state_action_dict\n",
    "        \n",
    "\n",
    "        for episode in range(self.max_episodes):\n",
    "            current_state = initial_state\n",
    "            current_action = initial_action\n",
    "            (policy, state_action_value_dict) = self.update_one_episode(current_state, \\\n",
    "                                                                        current_action, \\\n",
    "                                                                        copy.deepcopy(state_action_value_dict), \\\n",
    "                                                                        copy.deepcopy(policy))\n",
    "            \n",
    "\n",
    "        return (policy, state_action_value_dict)\n",
    "    \n",
    "    \n",
    "    # a helper function used to calculate the G_t_lambd value for current state\n",
    "    def calculate_g_t_lambd(step, g_t_list, lambd):\n",
    "        total_step = len(g_t_list)\n",
    "        g_t_lambd = 0\n",
    "        current_coeff = 1\n",
    "        for i in range(step, total_step, 1):\n",
    "            g_t_lambd += current_coeff * g_t_list[i]\n",
    "            current_coeff *= lamba\n",
    "        return (1.0 - lambd) * g_t_lambd\n",
    "    \n",
    "    def get_one_TDLambda_update(self, pol: Policy, state_value_dict, action_generator_for_each_state):\n",
    "        # Update MDP by running one episode of TD-lambda learning\n",
    "        \n",
    "        \n",
    "        # preserve the state value/ action information of the current MDP\n",
    "        state_action_dict = self.mdp.state_action_dict\n",
    "        state_value = copy.deepcopy(state_value_dict)\n",
    "        \n",
    "        # generate the trace of this episode\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        value_list = []\n",
    "        \n",
    "        # use the initial state generate by MDP, it could also be initialized to a fixed state\n",
    "        current_state = self.mdp.init_state_generator()\n",
    "        \n",
    "        for step in range(self.max_steps):\n",
    "            action_to_take = action_generator_for_each_state[current_state]()\n",
    "            next_state, reward = self.mdp.state_reward_gen_dict[current_state][action_to_take]()\n",
    "            state_list.append(current_state)\n",
    "            action_list.append(action_to_take)\n",
    "            value_list.append(reward)\n",
    "            if current_state in self.mdp.terminal_states:\n",
    "                break\n",
    "            else:\n",
    "                # increment by one step\n",
    "                current_state = next_state\n",
    "        \n",
    "        g_t_list = []\n",
    "        # calculating G_t in a backward manner\n",
    "        for i in range(len(state_list) - 1, -1. -1):\n",
    "            if i == len(state_list) - 1:\n",
    "                current_g_t = value_list[i]\n",
    "            else:\n",
    "                current_g_t = value_list[i] + g_t_list[-1] * self.lambd\n",
    "            g_t_list.append(current_g_t)\n",
    "        # reverse the g_t_list to match the time\n",
    "        g_t_list = g_t_list[::-1]\n",
    "        \n",
    "        num_of_steps = len(state_list)\n",
    "        E = np.zeros((num_of_steps, hidden_size))\n",
    "        #  update the value function and at the same time update the w\n",
    "        for step in range(len(state_list)):\n",
    "            # this time only consider the state visited in this episode\n",
    "            g_t_lambd = calculate_g_t_lambd(step, g_t_list, lambd)\n",
    "            state_value[state_list[step]] += self.learning_rate * (g_t_lambd - state_value[state_list[step]])\n",
    "            \n",
    "            # updating the w value at the same time\n",
    "            current_state = state_list[step]\n",
    "#             current_value = G_t_lambd[step] + np.dot(w, state_list[step + 1]) # R + one-step looking ahead\n",
    "            \n",
    "            v_hat = np.dot(self.w, current_state)\n",
    "            delta_t = alpha * (current_value - v_hat)*current_state\n",
    "            E = gamma * lambd * E\n",
    "            E[step] += current_state\n",
    "            dw = alpha * delta_t * E[step]\n",
    "            w = self.w - dw\n",
    "            self.w = w\n",
    "        \n",
    "        return state_value\n",
    "    \n",
    "    # the update function running over all the episodes\n",
    "    def update(self, pol:Policy):\n",
    "        state_action_dict = self.mdp.state_action_dict\n",
    "        # initate the value for each state as 0\n",
    "        V_s = {s: 0. for s in sa_dict.keys()}\n",
    "        \n",
    "        # action generator for each state\n",
    "        action_generator_for_each_state = {s: random_generator_based_on_prob(pol.get_state_probabilities(s))\n",
    "                        for s in sa_dict.keys()}\n",
    "        episodes = 0\n",
    "        updates = 0\n",
    "\n",
    "        for episode in range(self.max_episodes):\n",
    "            et_dict = {s: 0. for s in sa_dict.keys()}\n",
    "            state = self.mdp_rep.init_state_gen()\n",
    "            steps = 0\n",
    "            terminate = False\n",
    "\n",
    "            V_s = self.get_one_TDLambda_update(pol, copy.deepcopy(V_s), action_generator_for_each_state)\n",
    "\n",
    "        return V_s\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Implement Q-Learning with Value Function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# the idea is similar, by the time updating the value function, using function approximation\n",
    "# rather than the tabular settings from the MDP\n",
    "class Q_leaning:\n",
    "\n",
    "    def __init__(self, mdp, epsilon: float, learning_rate: float, learning_rate_decay: float, \\\n",
    "                 lambd: float, num_episodes: int, max_steps: int, initial_policy, hidden_size, w = None):\n",
    "\n",
    "        self.mdp = mdp,\n",
    "        self.lambd = lambd\n",
    "        self.epsilon = epsilon,\n",
    "        self.num_episodes = num_episodes,\n",
    "        self.max_steps = max_steps\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.gamma_lambda = self.mdp.gamma * lambd\n",
    "        self.policy = initial_policy\n",
    "        if w is None:\n",
    "            self.w = np.random.rand(hidden_size)\n",
    "        else:\n",
    "            self.w = w\n",
    "            \n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def epsilon_greedy(self, policy, current_state, state_value_dict, epsilon):\n",
    "        # the probability of selecting certain action based on the policy\n",
    "        actions = policy.get_state_probabilities(current_state)\n",
    "        \n",
    "        # the action chosen based on greedy algorithm\n",
    "        max_act = current_state\n",
    "        for action in actions:\n",
    "            if state_value_dict[current_state][action] > state_value_dict[current_state][max_act]:\n",
    "                max_act = action\n",
    "        \n",
    "        m = len(action_value_dict)\n",
    "        policy.edit_state_action_to_epsilon_greedy(current_state, self.mdp.action_value_dict)\n",
    "        if epsilon == 0:\n",
    "            return {max_act: 1.}\n",
    "        else:\n",
    "            # with 1 / epsilon to select a random  \n",
    "            return {action: epsilon / m + (1. - epsilon if a == max_act else 0.) for action in action_value_dict.keys()}\n",
    "    \n",
    "    # compared with SARSA, Q-learning has a separate method to choose next_action based on greedy policy\n",
    "    def Q_learning_greedy(self, policy, current_state, state_value_dict):\n",
    "        # the probability of selecting certain action based on the policy\n",
    "        actions = policy.get_state_probabilities(current_state)\n",
    "        \n",
    "        # the action chosen based on greedy algorithm\n",
    "        max_act = current_state\n",
    "        for action in actions:\n",
    "            if state_value_dict[current_state][action] > state_value_dict[current_state][max_act]:\n",
    "                max_act = action\n",
    "        \n",
    "        policy = policy.update_state_action_to_greedy(current_state, self.mdp.action_value_dict)\n",
    "        return {max_act: 1.}\n",
    "\n",
    "        \n",
    "    def update_one_episode(self, current_state, current_action, state_action_value_dict, policy):\n",
    "        # Use Q leaning to update the state_action_dictiary and change the policy\n",
    "        curt_state = current_state\n",
    "        online_state_value_dict = copy.deepcopy(state_action_value_dict)\n",
    "        curt_action = current_action\n",
    "        for step in range(self.max_steps):\n",
    "            # generate next step based on epsilon-greedy\n",
    "            actions = self.Q_learning_greedy(policy, current_state, online_state_value_dict)\n",
    "            # here the next action to take is based on the probability generated by the epsilon greedy\n",
    "            next_action_generator = random_generator_based_on_prob(actions)\n",
    "            action_to_take = next_action_generator()\n",
    "            \n",
    "            next_state, reward = self.mdp.state_reward_gen_dict[current_state][curt_action]()\n",
    "            \n",
    "            # online update for Q(S,A)\n",
    "            online_state_value_dict[curt_state][curt_action] = online_state_value_dict[curt_state][curt_action] + \\\n",
    "            self.learning_rate * (reward + self.gamma * online_state_value_dict[next_state][action_to_take] - online_state_value_dict[curt_state][curt_action])\n",
    "            if curt_state in self.mdp.terminal_states:\n",
    "                break\n",
    "            else:\n",
    "                curt_state = next_state\n",
    "                curt_action = action_to_take\n",
    "        # update the policy and the Q(S,A) accordingly\n",
    "        return (policy, online_state_value_dict)\n",
    "    \n",
    "    def Q_leaning_update(self, pol:Policy, inital_state, intial_action):\n",
    "        \"\"\"\n",
    "        Update the policy and Q(S,A) using SARSA online update\n",
    "        \"\"\"\n",
    "        \n",
    "        state_action_value_dict = self.mdp.state_action_dict\n",
    "        \n",
    "\n",
    "        for episode in range(self.max_episodes):\n",
    "            current_state = initial_state\n",
    "            current_action = initial_action\n",
    "            (policy, state_action_value_dict) = self.update_one_episode(current_state, \\\n",
    "                                                                        current_action, \\\n",
    "                                                                        copy.deepcopy(state_action_value_dict), \\\n",
    "                                                                        copy.deepcopy(policy))\n",
    "            \n",
    "\n",
    "        return (policy, state_action_value_dict)\n",
    "    \n",
    "    \n",
    "    # a helper function used to calculate the G_t_lambd value for current state\n",
    "    def calculate_g_t_lambd(step, g_t_list, lambd):\n",
    "        total_step = len(g_t_list)\n",
    "        g_t_lambd = 0\n",
    "        current_coeff = 1\n",
    "        for i in range(step, total_step, 1):\n",
    "            g_t_lambd += current_coeff * g_t_list[i]\n",
    "            current_coeff *= lamba\n",
    "        return (1.0 - lambd) * g_t_lambd\n",
    "    \n",
    "    def get_one_update(self, pol: Policy, state_value_dict, action_generator_for_each_state):\n",
    "        # Update MDP by running one episode of TD-lambda learning\n",
    "        \n",
    "        \n",
    "        # preserve the state value/ action information of the current MDP\n",
    "        state_action_dict = self.mdp.state_action_dict\n",
    "        state_value = copy.deepcopy(state_value_dict)\n",
    "        \n",
    "        # generate the trace of this episode\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        value_list = []\n",
    "        \n",
    "        # use the initial state generate by MDP, it could also be initialized to a fixed state\n",
    "        current_state = self.mdp.init_state_generator()\n",
    "        \n",
    "        for step in range(self.max_steps):\n",
    "            action_to_take = action_generator_for_each_state[current_state]()\n",
    "            next_state, reward = self.mdp.state_reward_gen_dict[current_state][action_to_take]()\n",
    "            state_list.append(current_state)\n",
    "            action_list.append(action_to_take)\n",
    "            value_list.append(reward)\n",
    "            if current_state in self.mdp.terminal_states:\n",
    "                break\n",
    "            else:\n",
    "                # increment by one step\n",
    "                current_state = next_state\n",
    "        \n",
    "        g_t_list = []\n",
    "        # calculating G_t in a backward manner\n",
    "        for i in range(len(state_list) - 1, -1. -1):\n",
    "            if i == len(state_list) - 1:\n",
    "                current_g_t = value_list[i]\n",
    "            else:\n",
    "                current_g_t = value_list[i] + g_t_list[-1] * self.lambd\n",
    "            g_t_list.append(current_g_t)\n",
    "        # reverse the g_t_list to match the time\n",
    "        g_t_list = g_t_list[::-1]\n",
    "        E = np.zeros(hidden_size)\n",
    "        #  update the value function \n",
    "        for step in range(len(state_list)):\n",
    "            # this time only consider the state visited in this episode\n",
    "            g_t_lambd = calculate_g_t_lambd(step, g_t_list, lambd)\n",
    "            state_value[state_list[step]] += self.learning_rate * (g_t_lambd - state_value[state_list[step]])\n",
    "            current_state = state_list[step]\n",
    "            v_hat = np.dot(self.w, current_state)\n",
    "            delta_t = alpha * (current_value - v_hat)*current_state\n",
    "            E = gamma * lambd * E\n",
    "            E += current_state\n",
    "            dw = alpha * delta_t * E\n",
    "            w = self.w - dw\n",
    "            self.w = w\n",
    "        return state_value\n",
    "    \n",
    "    # the update function running over all the episodes\n",
    "    def update(self, pol:Policy):\n",
    "        state_action_dict = self.mdp.state_action_dict\n",
    "        # initate the value for each state as 0\n",
    "        V_s = {s: 0. for s in sa_dict.keys()}\n",
    "        \n",
    "        # action generator for each state\n",
    "        action_generator_for_each_state = {s: random_generator_based_on_prob(pol.get_state_probabilities(s))\n",
    "                        for s in sa_dict.keys()}\n",
    "        episodes = 0\n",
    "        updates = 0\n",
    "\n",
    "        for episode in range(self.max_episodes):\n",
    "            et_dict = {s: 0. for s in sa_dict.keys()}\n",
    "            state = self.mdp_rep.init_state_gen()\n",
    "            steps = 0\n",
    "            terminate = False\n",
    "\n",
    "            V_s = self.get_one_TDLambda_update(pol, copy.deepcopy(V_s), action_generator_for_each_state)\n",
    "\n",
    "        return V_s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
