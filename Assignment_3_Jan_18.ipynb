{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS&E 346 Assignment 3\n",
    "#### January 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eig\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "Write code for Policy Evaluation (tabular) algorithm\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: evaluate a given policy ${\\pi}$  \n",
    "Solution: iteratively application of Bellman expectation backup\n",
    "Using synchronus backups:\n",
    "* At each iteration k + 1\n",
    "* For all states ${s \\in S}$ , update ${v_{k+1}(s)}$ from ${v_{k}(s')}$\n",
    "* where ${s'}$ is a successor state of s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the tabular states/ policies are represented as a matrix, the update function can be written as follows\n",
    "# another implementation of the policy evaluation using the previous definition of MDP will be used later\n",
    "class MRP:\n",
    "    \"\"\"Iterative policy evaluation\"\"\"\n",
    "    def iterative_policy_evaluation(self):\n",
    "        # initiate the value of the state to zeros\n",
    "        reward = self.reward\n",
    "        transition = self.transition\n",
    "        value = self.value\n",
    "        gamma = self.gamma\n",
    "        value = np.zeros(self.reward.shape)\n",
    "        \n",
    "        while True:\n",
    "            # iteratively update the value function of each state until the value of state no longer changes\n",
    "            old_v = copy.deepcopy(value)\n",
    "            value = reward + gamma * np.matmul(transition, value)\n",
    "            if np.sum(np.abs(value - old_v)) < 1e-8:\n",
    "                # use L1 norm to measure the distance\n",
    "                break\n",
    "        self.value = value\n",
    "                    \n",
    "        \n",
    "    def __init__(self, reward, transition, value, gamma):\n",
    "        \"\"\"\n",
    "        In the Markovian reward process, the reward is a function of state\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.transition = transition\n",
    "        self.value = value\n",
    "        self.gamma = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A toy example of a Mars-Rover, where the reward space is defined as $${[1, 0, 0, 0, 0, 0, 10]}$$, and the transition probability of each state is it has 50% chance to go to the left state, and 50% chance going to the right state, assume gamma is defined as 0.95. The Markov reward process can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.matrix([[1, 0, 0, 0, 0, 0, 10]]).T\n",
    "transition = np.matrix([[1, 0, 0, 0, 0, 0, 0],\n",
    "                        [0.5, 0, 0.5, 0, 0, 0, 0], \n",
    "                        [0, 0.5, 0, 0.5, 0, 0, 0], \n",
    "                        [0, 0, 0.5, 0, 0.5, 0, 0], \n",
    "                        [0, 0, 0, 0.5, 0, 0.5, 0], \n",
    "                        [0, 0, 0, 0, 0.5, 0, 0.5], \n",
    "                        [0, 0, 0, 0, 0, 0, 1]])\n",
    "value = np.random.rand(reward.shape[0], reward.shape[1])\n",
    "gamma = 0.95\n",
    "mrp_process = MRP(reward, transition, value, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.        ]\n",
      " [ 33.5282314 ]\n",
      " [ 50.58575033]\n",
      " [ 72.96808508]\n",
      " [103.0312709 ]\n",
      " [143.93985365]\n",
      " [199.99999995]]\n"
     ]
    }
   ],
   "source": [
    "mrp_process.iterative_policy_evaluation()\n",
    "print(mrp_process.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "Write code for Policy Iteration (tabular) algorithm\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy improvement:\n",
    "Consider a deterministic policy: ${a = \\pi(s)}$, we can improve the policy by acting greedily, $${\\pi'(s) = argmax_{a \\in A} q_\\pi (s, a) }$$  \n",
    "This improves the value from any state s over one time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a Markov Desicion Process where the policy and reward are deterministic\n",
    "class MDP:\n",
    "    \n",
    "    \"\"\"Iterative policy evaluation\"\"\"\n",
    "    def iterative_policy_evaluation(self):\n",
    "        # initiate the value of the state to zeros\n",
    "        reward = self.reward\n",
    "        transition = self.transition\n",
    "        value = self.value\n",
    "        gamma = self.gamma\n",
    "        value = np.zeros(self.reward.shape)\n",
    "        \n",
    "        while True:\n",
    "            # iteratively update the value function of each state until the value of state no longer changes\n",
    "            old_v = copy.deepcopy(value)\n",
    "            value = reward + gamma * np.matmul(transition, value)\n",
    "            if np.sum(np.abs(value - old_v)) < 1e-8:\n",
    "                # use L1 norm to measure the distance\n",
    "                break\n",
    "        self.value = value\n",
    "        \n",
    "    def construct_policy_set(self, policy):\n",
    "        # mapping a policy to a integer and process it later\n",
    "        policy_set = {}\n",
    "        for key, value in policy.items():\n",
    "            policy_set[key] = value\n",
    "        return policy_set\n",
    "    \n",
    "    def construct_policy_matrix(self):\n",
    "        self.policy_matrix = np.random.randint(len(self.policy_set), size = self.reward.shape)\n",
    "    \n",
    "    def argmax_policy(self, policy_matrix, i):\n",
    "        # This function is left abstract and when initiate a MDP object, this should be overwritten by an implementation\n",
    "        best_policy = -1\n",
    "        best_v = - math.inf\n",
    "        for key, value in self.policy_set.items():\n",
    "            curt_value = 0\n",
    "            if key == 0:\n",
    "                if i > 0:\n",
    "                    curt_value = self.reward[i - 1] + self.value[i - 1]\n",
    "                else:\n",
    "                    # absorbing state\n",
    "                    curt_value = self.reward[i] + self.value[i]\n",
    "            elif key == 1:\n",
    "                if i < policy_matrix.shape[0] - 1:\n",
    "                    curt_value = self.reward[i + 1] + self.value[i + 1]\n",
    "                else:\n",
    "                    curt_value = self.reward[i] + self.value[i]\n",
    "            else:\n",
    "                if i != 0 and i < policy_matrix.shape[0] - 1:\n",
    "                    curt_value = 0.5 * (self.reward[i - 1] + self.value[i - 1]) + 0.5 * (self.reward[i + 1] + self.value[i + 1])\n",
    "                else:\n",
    "                    curt_value = self.reward[i] + self.value[i]\n",
    "            if best_v <= curt_value:\n",
    "                best_v = curt_value\n",
    "                best_policy = key\n",
    "        if best_v != -math.inf:\n",
    "            self.value[i] = best_v\n",
    "            policy_matrix[i] = best_policy\n",
    "        return policy_matrix[i]\n",
    "    \n",
    "    \"\"\"Policy Iteration\"\"\"\n",
    "    def policy_iteration(self):\n",
    "        # initialize the policy matrix with a random policy\n",
    "        policy_matrix = np.random.randint(len(self.policy_set), size = self.reward.shape)\n",
    "        print('policy matrix', policy_matrix)\n",
    "        value = np.zeros(self.reward.shape)\n",
    "        while True:\n",
    "            old_policy = copy.deepcopy(policy_matrix)\n",
    "            for i in range(policy_matrix.shape[0]):\n",
    "                policy_matrix[i] = self.argmax_policy(policy_matrix, i)\n",
    "            # compare the result from the update\n",
    "            if (np.sum(np.abs(old_policy - policy_matrix)) == 0):\n",
    "                break\n",
    "        self.policy_matrix = copy.deepcopy(policy_matrix)\n",
    "            \n",
    "        \n",
    "    def __init__(self, reward, transition, value, gamma, policy):\n",
    "        self.reward = reward\n",
    "        self.transition = transition\n",
    "        self.value = value\n",
    "        self.gamma = gamma\n",
    "        self.policy = policy\n",
    "        self.policy_set = self.construct_policy_set(policy)\n",
    "        # initialize the policy randomly\n",
    "        self.policy_matrix = self.construct_policy_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the similar Mars-Rover example as before:\n",
    "A toy example of a Mars-Rover, where the reward space is defined as $${[1, -1, -1, -1, -1, -1, 10]}$$, and the transition probability of each state is it has 50% chance to go to the left state, and 50% chance going to the right state, assume gamma is defined as 0.95. This time, we have 3 policies, namely, walk to the left for sure, walk to the right for sure, walk to the left/ right w/ 50% probability each.\n",
    "\n",
    "The Markov reward process can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.matrix([[1, -1, -1, -1, -1, -1, 10]]).T\n",
    "transition = np.matrix([[1, 0, 0, 0, 0, 0, 0],\n",
    "                        [0.5, 0, 0.5, 0, 0, 0, 0], \n",
    "                        [0, 0.5, 0, 0.5, 0, 0, 0], \n",
    "                        [0, 0, 0.5, 0, 0.5, 0, 0], \n",
    "                        [0, 0, 0, 0.5, 0, 0.5, 0], \n",
    "                        [0, 0, 0, 0, 0.5, 0, 0.5], \n",
    "                        [0, 0, 0, 0, 0, 0, 1]])\n",
    "value = np.random.rand(reward.shape[0], reward.shape[1])\n",
    "gamma = 0.95\n",
    "policy = {0:'To left', 1:'To right', 2:'50% go to left, 50% go to right'}\n",
    "mdp_process = MDP(reward, transition, value, gamma, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'To left', 1: 'To right', 2: '50% go to left, 50% go to right'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_process.policy_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy matrix [[1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "mdp_process.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_process.policy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "Write code for Value Iteration (tabular) algorithm\n",
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: find the optimal policy ${\\pi}$  \n",
    "Solution: iteratively application of Bellman optimality backup \n",
    "Using synchronous backups:\n",
    "* At each iteration k + 1, for all states ${s \\in S}$\n",
    "* Update ${v_{k+1}(s)}$ from ${v_k(s')}$\n",
    "$${v_{k + 1} = max_{a \\in A}(R^a + \\gamma P^a v_{k})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a Markov Desicion Process where the policy and reward are deterministic\n",
    "class MDP:\n",
    "    \n",
    "    \"\"\"Iterative policy evaluation\"\"\"\n",
    "    def iterative_policy_evaluation(self):\n",
    "        # initiate the value of the state to zeros\n",
    "        reward = self.reward\n",
    "        transition = self.transition\n",
    "        value = self.value\n",
    "        gamma = self.gamma\n",
    "        value = np.zeros(self.reward.shape)\n",
    "        \n",
    "        while True:\n",
    "            # iteratively update the value function of each state until the value of state no longer changes\n",
    "            old_v = copy.deepcopy(value)\n",
    "            value = reward + gamma * np.matmul(transition, value)\n",
    "            if np.sum(np.abs(value - old_v)) < 1e-8:\n",
    "                # use L1 norm to measure the distance\n",
    "                break\n",
    "        self.value = value\n",
    "        \n",
    "    def construct_policy_set(self, policy):\n",
    "        # mapping a policy to a integer and process it later\n",
    "        policy_set = {}\n",
    "        for key, value in policy.items():\n",
    "            policy_set[key] = value\n",
    "        return policy_set\n",
    "    \n",
    "    def construct_policy_matrix(self):\n",
    "        self.policy_matrix = np.random.randint(len(self.policy_set), size = self.reward.shape)\n",
    "    \n",
    "    def argmax_policy(self, policy_matrix, i):\n",
    "        # This function is left abstract and when initiate a MDP object, this should be overwritten by an implementation\n",
    "        best_policy = -1\n",
    "        best_v = - math.inf\n",
    "        for key, value in self.policy_set.items():\n",
    "            curt_value = 0\n",
    "            if key == 0:\n",
    "                if i > 0:\n",
    "                    curt_value = self.reward[i - 1] + self.value[i - 1]\n",
    "                else:\n",
    "                    # absorbing state\n",
    "                    curt_value = self.reward[i] + self.value[i]\n",
    "            elif key == 1:\n",
    "                if i < policy_matrix.shape[0] - 1:\n",
    "                    curt_value = self.reward[i + 1] + self.value[i + 1]\n",
    "                else:\n",
    "                    curt_value = self.reward[i] + self.value[i]\n",
    "            else:\n",
    "                if i != 0 and i < policy_matrix.shape[0] - 1:\n",
    "                    curt_value = 0.5 * (self.reward[i - 1] + self.value[i - 1]) + 0.5 * (self.reward[i + 1] + self.value[i + 1])\n",
    "                else:\n",
    "                    curt_value = self.reward[i] + self.value[i]\n",
    "            if best_v <= curt_value:\n",
    "                best_v = curt_value\n",
    "                best_policy = key\n",
    "        if best_v != -math.inf:\n",
    "            self.value[i] = best_v\n",
    "            policy_matrix[i] = best_policy\n",
    "        return policy_matrix[i]\n",
    "    \n",
    "    \"\"\"Policy Iteration\"\"\"\n",
    "    def policy_iteration(self):\n",
    "        # initialize the policy matrix with a random policy\n",
    "        policy_matrix = np.random.randint(len(self.policy_set), size = self.reward.shape)\n",
    "        print('policy matrix', policy_matrix)\n",
    "        value = np.zeros(self.reward.shape)\n",
    "        while True:\n",
    "            old_policy = copy.deepcopy(policy_matrix)\n",
    "            for i in range(policy_matrix.shape[0]):\n",
    "                policy_matrix[i] = self.argmax_policy(policy_matrix, i)\n",
    "            # compare the result from the update\n",
    "            if (np.sum(np.abs(old_policy - policy_matrix)) == 0):\n",
    "                break\n",
    "        self.policy_matrix = copy.deepcopy(policy_matrix)\n",
    "        \n",
    "    def value_iteration(self):\n",
    "        # the value matrix and the transition matrix are pre-defined before problem and are deterministic\n",
    "        value = self.value\n",
    "        transition = self.transition\n",
    "        while True:\n",
    "            old_v = copy.deepcopy(value)\n",
    "            for i in range(old_v.shape[0]):\n",
    "                value[i] = np.max(reward + self.gamma * np.dot(transition[i, :, :], value).reshape(-1, reward.shape[1]))\n",
    "            if np.sum(np.abs(old_v - value)) < 1e-8:\n",
    "                break;\n",
    "        self.value = value\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __init__(self, reward, transition, value, gamma, policy):\n",
    "        \"\"\"\n",
    "        By this definition, the reward is a n * m matrix with one state and one action\n",
    "        the transition matrix is a n * m * n matrix, and each value of the matrix is indicating the probability\n",
    "        of p(s'|s, a)\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.transition = transition\n",
    "        self.value = value\n",
    "        self.gamma = gamma\n",
    "        self.policy = policy\n",
    "        self.policy_set = self.construct_policy_set(policy)\n",
    "        # initialize the policy randomly\n",
    "        self.policy_matrix = self.construct_policy_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.matrix([[1, -1],[-1, 1]])\n",
    "transition = np.zeros((2,2,2))\n",
    "transition[0, 0, 0] = 0.7\n",
    "transition[0, 0, 1] = 0.3\n",
    "transition[0, 1, 0] = 0.3\n",
    "transition[0, 1, 1] = 0.7\n",
    "transition[1, 1, 0] = 0.4\n",
    "transition[1, 1, 1] = 0.6\n",
    "transition[1, 0, 0] = 0.6\n",
    "transition[1, 0, 1] = 0.4\n",
    "value = np.random.rand(reward.shape[0], reward.shape[1])\n",
    "gamma = 0.9\n",
    "policy = {0:'To left', 1:'To right', 2:'50% go to left, 50% go to right'}\n",
    "mdp_process = MDP(reward, transition, value, gamma, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
